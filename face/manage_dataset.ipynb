{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-25T18:34:28.027284Z",
     "start_time": "2025-05-25T18:34:19.810153Z"
    }
   },
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "from deepface import DeepFace\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\bryan\\anaconda3\\envs\\yolo_fall_detect\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T18:41:02.509606Z",
     "start_time": "2025-05-25T18:41:02.494224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configuración\n",
    "DATASET_PATH = \"dataset/cplfw/images\"\n",
    "SPLIT_SIZE = 10\n",
    "MODELS = [\"Facenet\", \"VGG-Face\", \"ArcFace\", \"GhostFaceNet\", \"OpenFace\"]\n",
    "\n",
    "def get_persons_and_images(dataset_path):\n",
    "    \"\"\"Devuelve un diccionario persona -> lista de imágenes ordenadas.\"\"\"\n",
    "    regex = re.compile(r\"(.+?)_(\\d+)\\.jpg$\")\n",
    "    persons = {}\n",
    "    for file in os.listdir(dataset_path):\n",
    "        match = regex.match(file)\n",
    "        if match:\n",
    "            name = match.group(1)\n",
    "            persons.setdefault(name, []).append(file)\n",
    "    for k in persons:\n",
    "        # Ordenar imágenes por número\n",
    "        persons[k].sort(key=lambda x: int(re.match(r\".+_(\\d+)\\.jpg$\", x).group(1)))\n",
    "    return persons\n",
    "\n",
    "def split_persons(persons, split_size):\n",
    "    \"\"\"Divide la lista de personas en bloques de tamaño split_size.\"\"\"\n",
    "    person_list = list(persons.keys())\n",
    "    return [person_list[i:i+split_size] for i in range(0, len(person_list), split_size)]\n",
    "\n",
    "def benchmark_block(model_name, block_persons, persons, dataset_path):\n",
    "    \"\"\"Evalúa un bloque de 10 personas usando un modelo de DeepFace.\"\"\"\n",
    "    references = {}\n",
    "    probes = []\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    times = []\n",
    "\n",
    "    # Prepara referencias\n",
    "    for person in block_persons:\n",
    "        ref_img = [img for img in persons[person] if \"_1.jpg\" in img]\n",
    "        if not ref_img:\n",
    "            continue\n",
    "        references[person] = os.path.join(dataset_path, ref_img[0])\n",
    "\n",
    "    # Prepara pruebas\n",
    "    for idx, person in enumerate(block_persons):\n",
    "        probe_imgs = [img for img in persons[person] if \"_1.jpg\" not in img]\n",
    "        for probe_img in probe_imgs:\n",
    "            probe_path = os.path.join(dataset_path, probe_img)\n",
    "            probes.append((probe_path, person))\n",
    "            true_labels.append(person)\n",
    "\n",
    "    # Inferencia\n",
    "    for probe_path, true_person in probes:\n",
    "        start = time.time()\n",
    "        best_match = None\n",
    "        min_dist = float(\"inf\")\n",
    "        for ref_person, ref_path in references.items():\n",
    "            try:\n",
    "                result = DeepFace.verify(img1_path=probe_path, img2_path=ref_path, model_name=model_name, enforce_detection=False, detector_backend=\"skip\")\n",
    "                dist = result[\"distance\"]\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    best_match = ref_person\n",
    "            except Exception as e:\n",
    "                print(f\"Error procesando {probe_path} vs {ref_path}: {e}\")\n",
    "                continue\n",
    "        pred_labels.append(best_match)\n",
    "        times.append(time.time() - start)\n",
    "\n",
    "    # Métricas\n",
    "    acc = accuracy_score(true_labels, pred_labels)\n",
    "    prec = precision_score(true_labels, pred_labels, average=\"macro\", zero_division=0)\n",
    "    recall = recall_score(true_labels, pred_labels, average=\"macro\", zero_division=0)\n",
    "    f1 = f1_score(true_labels, pred_labels, average=\"macro\", zero_division=0)\n",
    "    avg_time = np.mean(times)\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"avg_time\": avg_time,\n",
    "        \"n_tests\": len(probes)\n",
    "    }\n",
    "\n",
    "def run_benchmark():\n",
    "    persons = get_persons_and_images(DATASET_PATH)\n",
    "    blocks = split_persons(persons, SPLIT_SIZE)\n",
    "    results = []\n",
    "\n",
    "    for model in MODELS:\n",
    "        print(f\"Evaluando modelo: {model}\")\n",
    "        for i, block_persons in enumerate(blocks):\n",
    "            print(f\" - Split {i+1}/{len(blocks)}\")\n",
    "            metrics = benchmark_block(model, block_persons, persons, DATASET_PATH)\n",
    "            metrics[\"split\"] = i+1\n",
    "            results.append(metrics)\n",
    "            print(metrics)\n",
    "\n",
    "    pd.DataFrame(results).to_csv(\"benchmark_results.csv\", index=False)"
   ],
   "id": "fdc55b2783f19b8a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T18:41:20.964909Z",
     "start_time": "2025-05-25T18:41:06.224982Z"
    }
   },
   "cell_type": "code",
   "source": "run_benchmark()",
   "id": "f94bf34a02e6b1e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando modelo: VGG-Face\n",
      " - Split 1/393\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[32m~\\AppData\\Local\\Temp\\ipykernel_22056\\199647583.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m run_benchmark()\n",
      "\u001B[32m~\\AppData\\Local\\Temp\\ipykernel_22056\\106812517.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m     91\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m model \u001B[38;5;28;01min\u001B[39;00m MODELS:\n\u001B[32m     92\u001B[39m         print(f\"Evaluando modelo: {model}\")\n\u001B[32m     93\u001B[39m         \u001B[38;5;28;01mfor\u001B[39;00m i, block_persons \u001B[38;5;28;01min\u001B[39;00m enumerate(blocks):\n\u001B[32m     94\u001B[39m             print(f\" - Split {i+\u001B[32m1\u001B[39m}/{len(blocks)}\")\n\u001B[32m---> \u001B[39m\u001B[32m95\u001B[39m             metrics = benchmark_block(model, block_persons, persons, DATASET_PATH)\n\u001B[32m     96\u001B[39m             metrics[\u001B[33m\"split\"\u001B[39m] = i+\u001B[32m1\u001B[39m\n\u001B[32m     97\u001B[39m             results.append(metrics)\n\u001B[32m     98\u001B[39m             print(metrics)\n",
      "\u001B[32m~\\AppData\\Local\\Temp\\ipykernel_22056\\106812517.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(model_name, block_persons, persons, dataset_path)\u001B[39m\n\u001B[32m     60\u001B[39m                 dist = result[\u001B[33m\"distance\"\u001B[39m]\n\u001B[32m     61\u001B[39m                 \u001B[38;5;28;01mif\u001B[39;00m dist < min_dist:\n\u001B[32m     62\u001B[39m                     min_dist = dist\n\u001B[32m     63\u001B[39m                     best_match = ref_person\n\u001B[32m---> \u001B[39m\u001B[32m64\u001B[39m             \u001B[38;5;28;01mexcept\u001B[39;00m Exception \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m     65\u001B[39m                 print(f\"Error procesando {probe_path} vs {ref_path}: {e}\")\n\u001B[32m     66\u001B[39m                 \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[32m     67\u001B[39m         pred_labels.append(best_match)\n",
      "\u001B[32m~\\anaconda3\\envs\\yolo_fall_detect\\Lib\\site-packages\\deepface\\DeepFace.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(img1_path, img2_path, model_name, detector_backend, distance_metric, enforce_detection, align, expand_percentage, normalization, silent, threshold, anti_spoofing)\u001B[39m\n\u001B[32m    146\u001B[39m \n\u001B[32m    147\u001B[39m         - \u001B[33m'time'\u001B[39m (float): Time taken \u001B[38;5;28;01mfor\u001B[39;00m the verification process \u001B[38;5;28;01min\u001B[39;00m seconds.\n\u001B[32m    148\u001B[39m     \"\"\"\n\u001B[32m    149\u001B[39m \n\u001B[32m--> \u001B[39m\u001B[32m150\u001B[39m     return verification.verify(\n\u001B[32m    151\u001B[39m         img1_path=img1_path,\n\u001B[32m    152\u001B[39m         img2_path=img2_path,\n\u001B[32m    153\u001B[39m         model_name=model_name,\n",
      "\u001B[32m~\\anaconda3\\envs\\yolo_fall_detect\\Lib\\site-packages\\deepface\\modules\\verification.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(img1_path, img2_path, model_name, detector_backend, distance_metric, enforce_detection, align, expand_percentage, normalization, silent, threshold, anti_spoofing)\u001B[39m\n\u001B[32m    177\u001B[39m             \u001B[38;5;28;01mexcept\u001B[39;00m ValueError \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[32m    178\u001B[39m                 \u001B[38;5;28;01mraise\u001B[39;00m ValueError(f\"Exception while processing img{index}_path\") \u001B[38;5;28;01mfrom\u001B[39;00m err\n\u001B[32m    179\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m img_embeddings, img_facial_areas\n\u001B[32m    180\u001B[39m \n\u001B[32m--> \u001B[39m\u001B[32m181\u001B[39m     img1_embeddings, img1_facial_areas = extract_embeddings_and_facial_areas(img1_path, \u001B[32m1\u001B[39m)\n\u001B[32m    182\u001B[39m     img2_embeddings, img2_facial_areas = extract_embeddings_and_facial_areas(img2_path, \u001B[32m2\u001B[39m)\n\u001B[32m    183\u001B[39m \n\u001B[32m    184\u001B[39m     min_distance, min_idx, min_idy = float(\u001B[33m\"inf\"\u001B[39m), \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[32m~\\anaconda3\\envs\\yolo_fall_detect\\Lib\\site-packages\\deepface\\modules\\verification.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(img_path, index)\u001B[39m\n\u001B[32m    173\u001B[39m                     expand_percentage=expand_percentage,\n\u001B[32m    174\u001B[39m                     normalization=normalization,\n\u001B[32m    175\u001B[39m                     anti_spoofing=anti_spoofing,\n\u001B[32m    176\u001B[39m                 )\n\u001B[32m--> \u001B[39m\u001B[32m177\u001B[39m             \u001B[38;5;28;01mexcept\u001B[39;00m ValueError \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[32m    178\u001B[39m                 \u001B[38;5;28;01mraise\u001B[39;00m ValueError(f\"Exception while processing img{index}_path\") \u001B[38;5;28;01mfrom\u001B[39;00m err\n\u001B[32m    179\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m img_embeddings, img_facial_areas\n",
      "\u001B[32m~\\anaconda3\\envs\\yolo_fall_detect\\Lib\\site-packages\\deepface\\modules\\verification.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(img_path, model_name, detector_backend, enforce_detection, align, expand_percentage, normalization, anti_spoofing)\u001B[39m\n\u001B[32m    244\u001B[39m     \u001B[38;5;66;03m# find embeddings for each face\u001B[39;00m\n\u001B[32m    245\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m img_obj \u001B[38;5;28;01min\u001B[39;00m img_objs:\n\u001B[32m    246\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m anti_spoofing \u001B[38;5;28;01mis\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m \u001B[38;5;28;01mand\u001B[39;00m img_obj.get(\u001B[33m\"is_real\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;28;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[32m    247\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m ValueError(\u001B[33m\"Spoof detected in given image.\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m248\u001B[39m         img_embedding_obj = representation.represent(\n\u001B[32m    249\u001B[39m             img_path=img_obj[\u001B[33m\"face\"\u001B[39m],\n\u001B[32m    250\u001B[39m             model_name=model_name,\n\u001B[32m    251\u001B[39m             enforce_detection=enforce_detection,\n",
      "\u001B[32m~\\anaconda3\\envs\\yolo_fall_detect\\Lib\\site-packages\\deepface\\modules\\representation.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(img_path, model_name, enforce_detection, detector_backend, align, expand_percentage, normalization, anti_spoofing, max_faces)\u001B[39m\n\u001B[32m    129\u001B[39m \n\u001B[32m    130\u001B[39m         \u001B[38;5;66;03m# custom normalization\u001B[39;00m\n\u001B[32m    131\u001B[39m         img = preprocessing.normalize_input(img=img, normalization=normalization)\n\u001B[32m    132\u001B[39m \n\u001B[32m--> \u001B[39m\u001B[32m133\u001B[39m         embedding = model.forward(img)\n\u001B[32m    134\u001B[39m \n\u001B[32m    135\u001B[39m         resp_objs.append(\n\u001B[32m    136\u001B[39m             {\n",
      "\u001B[32m~\\anaconda3\\envs\\yolo_fall_detect\\Lib\\site-packages\\deepface\\models\\facial_recognition\\VGGFace.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(self, img)\u001B[39m\n\u001B[32m     62\u001B[39m         \u001B[38;5;66;03m# embedding = model.predict(img, verbose=0)[0].tolist()\u001B[39;00m\n\u001B[32m     63\u001B[39m \n\u001B[32m     64\u001B[39m         \u001B[38;5;66;03m# having normalization layer in descriptor troubles for some gpu users (e.g. issue 957, 966)\u001B[39;00m\n\u001B[32m     65\u001B[39m         \u001B[38;5;66;03m# instead we are now calculating it with traditional way not with keras backend\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m66\u001B[39m         embedding = self.model(img, training=\u001B[38;5;28;01mFalse\u001B[39;00m).numpy()[\u001B[32m0\u001B[39m].tolist()\n\u001B[32m     67\u001B[39m         embedding = verification.l2_normalize(embedding)\n\u001B[32m     68\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m embedding.tolist()\n",
      "\u001B[32m~\\anaconda3\\envs\\yolo_fall_detect\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m     68\u001B[39m             \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[32m     69\u001B[39m             \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[32m     70\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m e.with_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m     71\u001B[39m         \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m72\u001B[39m             \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "\u001B[32m~\\anaconda3\\envs\\yolo_fall_detect\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    584\u001B[39m                 super().__call__(inputs, *copied_args, **copied_kwargs)\n\u001B[32m    585\u001B[39m \n\u001B[32m    586\u001B[39m             layout_map_lib._map_subclass_model_variable(self, self._layout_map)\n\u001B[32m    587\u001B[39m \n\u001B[32m--> \u001B[39m\u001B[32m588\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m super().__call__(*args, **kwargs)\n",
      "\u001B[32m~\\anaconda3\\envs\\yolo_fall_detect\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m     68\u001B[39m             \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[32m     69\u001B[39m             \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[32m     70\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m e.with_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m     71\u001B[39m         \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m72\u001B[39m             \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "\u001B[32m~\\anaconda3\\envs\\yolo_fall_detect\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1138\u001B[39m \n\u001B[32m   1139\u001B[39m                 with autocast_variable.enable_auto_cast_variables(\n\u001B[32m   1140\u001B[39m                     self._compute_dtype_object\n\u001B[32m   1141\u001B[39m                 ):\n\u001B[32m-> \u001B[39m\u001B[32m1142\u001B[39m                     outputs = call_fn(inputs, *args, **kwargs)\n\u001B[32m   1143\u001B[39m \n\u001B[32m   1144\u001B[39m                 \u001B[38;5;28;01mif\u001B[39;00m self._activity_regularizer:\n\u001B[32m   1145\u001B[39m                     self._handle_activity_regularization(inputs, outputs)\n",
      "\u001B[32m~\\anaconda3\\envs\\yolo_fall_detect\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    154\u001B[39m                 new_e = e\n\u001B[32m    155\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m new_e.with_traceback(e.__traceback__) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    156\u001B[39m         \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    157\u001B[39m             \u001B[38;5;28;01mdel\u001B[39;00m signature\n\u001B[32m--> \u001B[39m\u001B[32m158\u001B[39m             \u001B[38;5;28;01mdel\u001B[39;00m bound_signature\n",
      "\u001B[32m~\\anaconda3\\envs\\yolo_fall_detect\\Lib\\site-packages\\tf_keras\\src\\engine\\functional.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(self, inputs, training, mask)\u001B[39m\n\u001B[32m    510\u001B[39m         Returns:\n\u001B[32m    511\u001B[39m             A tensor \u001B[38;5;28;01mif\u001B[39;00m there \u001B[38;5;28;01mis\u001B[39;00m a single output, \u001B[38;5;28;01mor\u001B[39;00m\n\u001B[32m    512\u001B[39m             a list of tensors \u001B[38;5;28;01mif\u001B[39;00m there are more than one outputs.\n\u001B[32m    513\u001B[39m         \"\"\"\n\u001B[32m--> \u001B[39m\u001B[32m514\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m self._run_internal_graph(inputs, training=training, mask=mask)\n",
      "\u001B[32m~\\anaconda3\\envs\\yolo_fall_detect\\Lib\\site-packages\\tf_keras\\src\\engine\\functional.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(self, inputs, training, mask)\u001B[39m\n\u001B[32m    667\u001B[39m                 \u001B[38;5;28;01mif\u001B[39;00m any(t_id \u001B[38;5;28;01mnot\u001B[39;00m \u001B[38;5;28;01min\u001B[39;00m tensor_dict \u001B[38;5;28;01mfor\u001B[39;00m t_id \u001B[38;5;28;01min\u001B[39;00m node.flat_input_ids):\n\u001B[32m    668\u001B[39m                     \u001B[38;5;28;01mcontinue\u001B[39;00m  \u001B[38;5;66;03m# Node is not computable, try skipping.\u001B[39;00m\n\u001B[32m    669\u001B[39m \n\u001B[32m    670\u001B[39m                 args, kwargs = node.map_arguments(tensor_dict)\n\u001B[32m--> \u001B[39m\u001B[32m671\u001B[39m                 outputs = node.layer(*args, **kwargs)\n\u001B[32m    672\u001B[39m \n\u001B[32m    673\u001B[39m                 \u001B[38;5;66;03m# Update tensor_dict.\u001B[39;00m\n\u001B[32m    674\u001B[39m                 for x_id, y in zip(\n",
      "\u001B[32m~\\anaconda3\\envs\\yolo_fall_detect\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m     68\u001B[39m             \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[32m     69\u001B[39m             \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[32m     70\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m e.with_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m     71\u001B[39m         \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m72\u001B[39m             \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "\u001B[32m~\\anaconda3\\envs\\yolo_fall_detect\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1138\u001B[39m \n\u001B[32m   1139\u001B[39m                 with autocast_variable.enable_auto_cast_variables(\n\u001B[32m   1140\u001B[39m                     self._compute_dtype_object\n\u001B[32m   1141\u001B[39m                 ):\n\u001B[32m-> \u001B[39m\u001B[32m1142\u001B[39m                     outputs = call_fn(inputs, *args, **kwargs)\n\u001B[32m   1143\u001B[39m \n\u001B[32m   1144\u001B[39m                 \u001B[38;5;28;01mif\u001B[39;00m self._activity_regularizer:\n\u001B[32m   1145\u001B[39m                     self._handle_activity_regularization(inputs, outputs)\n",
      "\u001B[32m~\\anaconda3\\envs\\yolo_fall_detect\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    154\u001B[39m                 new_e = e\n\u001B[32m    155\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m new_e.with_traceback(e.__traceback__) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    156\u001B[39m         \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    157\u001B[39m             \u001B[38;5;28;01mdel\u001B[39;00m signature\n\u001B[32m--> \u001B[39m\u001B[32m158\u001B[39m             \u001B[38;5;28;01mdel\u001B[39;00m bound_signature\n",
      "\u001B[32m~\\anaconda3\\envs\\yolo_fall_detect\\Lib\\site-packages\\tf_keras\\src\\layers\\convolutional\\base_conv.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(self, inputs)\u001B[39m\n\u001B[32m    285\u001B[39m             outputs = self._jit_compiled_convolution_op(\n\u001B[32m    286\u001B[39m                 inputs, tf.convert_to_tensor(self.kernel)\n\u001B[32m    287\u001B[39m             )\n\u001B[32m    288\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m289\u001B[39m             outputs = self.convolution_op(inputs, self.kernel)\n\u001B[32m    290\u001B[39m \n\u001B[32m    291\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m self.use_bias:\n\u001B[32m    292\u001B[39m             output_rank = outputs.shape.rank\n",
      "\u001B[32m~\\anaconda3\\envs\\yolo_fall_detect\\Lib\\site-packages\\tf_keras\\src\\layers\\convolutional\\base_conv.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(self, inputs, kernel)\u001B[39m\n\u001B[32m    257\u001B[39m             tf_padding = self.padding.upper()\n\u001B[32m    258\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    259\u001B[39m             tf_padding = self.padding\n\u001B[32m    260\u001B[39m \n\u001B[32m--> \u001B[39m\u001B[32m261\u001B[39m         return tf.nn.convolution(\n\u001B[32m    262\u001B[39m             inputs,\n\u001B[32m    263\u001B[39m             kernel,\n\u001B[32m    264\u001B[39m             strides=list(self.strides),\n",
      "\u001B[32m~\\anaconda3\\envs\\yolo_fall_detect\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    151\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m Exception \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    152\u001B[39m       filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001B[32m    153\u001B[39m       \u001B[38;5;28;01mraise\u001B[39;00m e.with_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    154\u001B[39m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m155\u001B[39m       \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "\u001B[32m~\\anaconda3\\envs\\yolo_fall_detect\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m   1257\u001B[39m \n\u001B[32m   1258\u001B[39m       \u001B[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001B[39;00m\n\u001B[32m   1259\u001B[39m       \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1260\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m dispatch_target(*args, **kwargs)\n\u001B[32m-> \u001B[39m\u001B[32m1261\u001B[39m       \u001B[38;5;28;01mexcept\u001B[39;00m (TypeError, ValueError):\n\u001B[32m   1262\u001B[39m         \u001B[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[39;00m\n\u001B[32m   1263\u001B[39m         \u001B[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001B[39;00m\n\u001B[32m   1264\u001B[39m         result = dispatch(op_dispatch_handler, args, kwargs)\n",
      "\u001B[32m~\\anaconda3\\envs\\yolo_fall_detect\\Lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(input, filters, strides, padding, data_format, dilations, name)\u001B[39m\n\u001B[32m   1182\u001B[39m     padding=\u001B[33m\"VALID\"\u001B[39m,\n\u001B[32m   1183\u001B[39m     data_format=\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1184\u001B[39m     dilations=\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1185\u001B[39m     name=\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m-> \u001B[39m\u001B[32m1186\u001B[39m   return convolution_internal(\n\u001B[32m   1187\u001B[39m       input,  \u001B[38;5;66;03m# pylint: disable=redefined-builtin\u001B[39;00m\n\u001B[32m   1188\u001B[39m       filters,\n\u001B[32m   1189\u001B[39m       strides=strides,\n",
      "\u001B[32m~\\anaconda3\\envs\\yolo_fall_detect\\Lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(input, filters, strides, padding, data_format, dilations, name, call_from_convolution, num_spatial_dims)\u001B[39m\n\u001B[32m   1315\u001B[39m         op = _conv3d_expanded_batch\n\u001B[32m   1316\u001B[39m       \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1317\u001B[39m         op = conv1d\n\u001B[32m   1318\u001B[39m \n\u001B[32m-> \u001B[39m\u001B[32m1319\u001B[39m       return op(\n\u001B[32m   1320\u001B[39m           input,\n\u001B[32m   1321\u001B[39m           filters,\n\u001B[32m   1322\u001B[39m           strides,\n",
      "\u001B[32m~\\anaconda3\\envs\\yolo_fall_detect\\Lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(input, filters, strides, padding, data_format, dilations, name)\u001B[39m\n\u001B[32m   2789\u001B[39m   input_rank = input.shape.rank\n\u001B[32m   2790\u001B[39m   \u001B[38;5;28;01mif\u001B[39;00m input_rank \u001B[38;5;28;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mor\u001B[39;00m input_rank < \u001B[32m5\u001B[39m:\n\u001B[32m   2791\u001B[39m     \u001B[38;5;66;03m# We avoid calling squeeze_batch_dims to reduce extra python function\u001B[39;00m\n\u001B[32m   2792\u001B[39m     \u001B[38;5;66;03m# call slowdown in eager mode.  This branch doesn't require reshapes.\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2793\u001B[39m     return gen_nn_ops.conv2d(\n\u001B[32m   2794\u001B[39m         input,\n\u001B[32m   2795\u001B[39m         filter=filters,\n\u001B[32m   2796\u001B[39m         strides=strides,\n",
      "\u001B[32m~\\anaconda3\\envs\\yolo_fall_detect\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001B[39m\n\u001B[32m   1338\u001B[39m         \u001B[33m\"dilations\"\u001B[39m, dilations)\n\u001B[32m   1339\u001B[39m       \u001B[38;5;28;01mreturn\u001B[39;00m _result\n\u001B[32m   1340\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m _core._NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m   1341\u001B[39m       _ops.raise_from_not_ok_status(e, name)\n\u001B[32m-> \u001B[39m\u001B[32m1342\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m _core._FallbackException:\n\u001B[32m   1343\u001B[39m       \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[32m   1344\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1345\u001B[39m       return conv2d_eager_fallback(\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T19:30:35.711418Z",
     "start_time": "2025-05-25T19:30:34.756026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# Cambia esta ruta a donde tengas las carpetas de LFW\n",
    "DATASET_PATH = \"dataset\\\\lfw\\\\lfw-deepfunneled\\\\lfw-deepfunneled\"\n",
    "\n",
    "def contar_sujetos_con_mas_de_10_fotos_por_carpeta(dataset_path, min_fotos=10):\n",
    "    sujetos = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
    "    sujetos_con_mas_de_min_fotos = 0\n",
    "    for sujeto in sujetos:\n",
    "        fotos = [f for f in os.listdir(os.path.join(dataset_path, sujeto)) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        if len(fotos) >= min_fotos:\n",
    "            sujetos_con_mas_de_min_fotos += 1\n",
    "    total_sujetos = len(sujetos)\n",
    "    return sujetos_con_mas_de_min_fotos, total_sujetos\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sujetos_10omas, total = contar_sujetos_con_mas_de_10_fotos_por_carpeta(DATASET_PATH, min_fotos=10)\n",
    "    print(f\"Sujetos con 10 o más fotos: {sujetos_10omas} / {total} ({100*sujetos_10omas/total:.2f}%)\")"
   ],
   "id": "2f96a62a3bc1d51e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sujetos con 10 o más fotos: 158 / 5749 (2.75%)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T19:30:04.326961Z",
     "start_time": "2025-05-25T19:30:04.265842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Cambia esta ruta a donde tengas las imágenes de LFW\n",
    "DATASET_PATH = \"dataset/cplfw/images\"\n",
    "\n",
    "def contar_sujetos_con_mas_de_3_fotos(dataset_path, min_fotos=3):\n",
    "    regex = re.compile(r\"(.+?)_\\d+\\.jpg$\")\n",
    "    sujetos = {}\n",
    "    for file in os.listdir(dataset_path):\n",
    "        match = regex.match(file)\n",
    "        if match:\n",
    "            nombre = match.group(1)\n",
    "            sujetos[nombre] = sujetos.get(nombre, 0) + 1\n",
    "    total_sujetos = len(sujetos)\n",
    "    sujetos_con_mas_de_min_fotos = sum(1 for count in sujetos.values() if count >= min_fotos)\n",
    "    return sujetos_con_mas_de_min_fotos, total_sujetos\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sujetos_3omas, total = contar_sujetos_con_mas_de_3_fotos(DATASET_PATH, min_fotos=3)\n",
    "    print(f\"Sujetos con 3 o más fotos: {sujetos_3omas} / {total} ({100*sujetos_3omas/total:.2f}%)\")"
   ],
   "id": "7b8681e3e8c5abcb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sujetos con 3 o más fotos: 3811 / 3929 (97.00%)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T19:35:52.371316Z",
     "start_time": "2025-05-25T19:35:48.155598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "LFW_PATH = \"dataset/lfw/lfw-deepfunneled/lfw-deepfunneled\"\n",
    "CPLFW_PATH = \"dataset/cplfw/images\"\n",
    "OUTPUT_PATH = \"dataset/merged_lfw_cplfw\"\n",
    "\n",
    "def obtener_imagenes_lfw(lfw_path, min_fotos=10):\n",
    "    sujetos_validos = {}\n",
    "    for sujeto in os.listdir(lfw_path):\n",
    "        path_sujeto = os.path.join(lfw_path, sujeto)\n",
    "        if os.path.isdir(path_sujeto):\n",
    "            fotos = sorted([f for f in os.listdir(path_sujeto) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "            if len(fotos) >= min_fotos:\n",
    "                sujetos_validos[sujeto] = fotos[:10]  # exactamente 10, las primeras en orden\n",
    "    return sujetos_validos\n",
    "\n",
    "def obtener_imagenes_cplfw(cplfw_path, sujetos_validos):\n",
    "    regex = re.compile(r\"(.+?)_(\\d+)\\.jpg$\")\n",
    "    sujetos_cplfw = {}\n",
    "    for file in os.listdir(cplfw_path):\n",
    "        match = regex.match(file)\n",
    "        if match:\n",
    "            nombre = match.group(1)\n",
    "            idx = int(match.group(2))\n",
    "            if nombre in sujetos_validos and idx >= 2:\n",
    "                sujetos_cplfw.setdefault(nombre, []).append(file)\n",
    "    # Solo nos quedamos con sujetos que tengan al menos una imagen >=_2\n",
    "    sujetos_cplfw = {k: v for k, v in sujetos_cplfw.items() if len(v) > 0}\n",
    "    return sujetos_cplfw\n",
    "\n",
    "def merge_lfw_cplfw(lfw_path, cplfw_path, min_lfw=10):\n",
    "    sujetos_lfw = obtener_imagenes_lfw(lfw_path, min_lfw)\n",
    "    sujetos_cplfw = obtener_imagenes_cplfw(cplfw_path, sujetos_lfw)\n",
    "    sujetos_comunes = set(sujetos_lfw.keys()) & set(sujetos_cplfw.keys())\n",
    "    sujetos_lfw = {k: sujetos_lfw[k] for k in sujetos_comunes}\n",
    "    sujetos_cplfw = {k: sujetos_cplfw[k] for k in sujetos_comunes}\n",
    "    return sujetos_lfw, sujetos_cplfw\n",
    "\n",
    "def copiar_imagenes_a_nueva_carpeta(lfw, cplfw, lfw_path, cplfw_path, output_path):\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    for sujeto in lfw:\n",
    "        out_sujeto_path = os.path.join(output_path, sujeto)\n",
    "        if not os.path.exists(out_sujeto_path):\n",
    "            os.makedirs(out_sujeto_path)\n",
    "        # Copia imágenes base (LFW)\n",
    "        for img in lfw[sujeto]:\n",
    "            src = os.path.join(lfw_path, sujeto, img)\n",
    "            dst = os.path.join(out_sujeto_path, \"lfw_\" + img)\n",
    "            shutil.copy2(src, dst)\n",
    "        # Copia imágenes de test (CPLFW)\n",
    "        for img in cplfw[sujeto]:\n",
    "            src = os.path.join(cplfw_path, img)\n",
    "            dst = os.path.join(out_sujeto_path, \"cplfw_\" + img)\n",
    "            shutil.copy2(src, dst)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    lfw, cplfw = merge_lfw_cplfw(LFW_PATH, CPLFW_PATH)\n",
    "    print(f\"Copiando sujetos en común: {len(lfw)}\")\n",
    "    copiar_imagenes_a_nueva_carpeta(lfw, cplfw, LFW_PATH, CPLFW_PATH, OUTPUT_PATH)\n",
    "    print(f\"¡Listo! Las imágenes se copiaron en {OUTPUT_PATH}\")"
   ],
   "id": "7536922589c6af4d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copiando sujetos en común: 135\n",
      "¡Listo! Las imágenes se copiaron en dataset/merged_lfw_cplfw\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T23:18:53.026735Z",
     "start_time": "2025-05-25T23:18:50.635356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import random\n",
    "\n",
    "LFW_PATH = \"dataset/lfw/lfw-deepfunneled/lfw-deepfunneled\"\n",
    "CPLFW_PATH = \"dataset/cplfw/images\"\n",
    "OUTPUT_PATH = \"dataset/merged_lfw_cplfw_50\"\n",
    "N_SUJETOS = 50\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "def obtener_imagenes_lfw(lfw_path, min_fotos=10):\n",
    "    sujetos_validos = {}\n",
    "    for sujeto in os.listdir(lfw_path):\n",
    "        path_sujeto = os.path.join(lfw_path, sujeto)\n",
    "        if os.path.isdir(path_sujeto):\n",
    "            fotos = sorted([f for f in os.listdir(path_sujeto) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "            if len(fotos) >= min_fotos:\n",
    "                sujetos_validos[sujeto] = fotos[:10]\n",
    "    return sujetos_validos\n",
    "\n",
    "def obtener_imagenes_cplfw(cplfw_path, sujetos_validos):\n",
    "    regex = re.compile(r\"(.+?)_(\\d+)\\.jpg$\")\n",
    "    sujetos_cplfw = {}\n",
    "    for file in os.listdir(cplfw_path):\n",
    "        match = regex.match(file)\n",
    "        if match:\n",
    "            nombre = match.group(1)\n",
    "            idx = int(match.group(2))\n",
    "            if nombre in sujetos_validos and idx >= 2:\n",
    "                sujetos_cplfw.setdefault(nombre, []).append(file)\n",
    "    sujetos_cplfw = {k: v for k, v in sujetos_cplfw.items() if len(v) > 0}\n",
    "    return sujetos_cplfw\n",
    "\n",
    "def merge_lfw_cplfw(lfw_path, cplfw_path, min_lfw=10):\n",
    "    sujetos_lfw = obtener_imagenes_lfw(lfw_path, min_lfw)\n",
    "    sujetos_cplfw = obtener_imagenes_cplfw(cplfw_path, sujetos_lfw)\n",
    "    sujetos_comunes = set(sujetos_lfw.keys()) & set(sujetos_cplfw.keys())\n",
    "    sujetos_lfw = {k: sujetos_lfw[k] for k in sujetos_comunes}\n",
    "    sujetos_cplfw = {k: sujetos_cplfw[k] for k in sujetos_comunes}\n",
    "    return sujetos_lfw, sujetos_cplfw\n",
    "\n",
    "def copiar_imagenes_a_nueva_carpeta(lfw, cplfw, lfw_path, cplfw_path, output_path):\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    for sujeto in lfw:\n",
    "        out_sujeto_path = os.path.join(output_path, sujeto)\n",
    "        if not os.path.exists(out_sujeto_path):\n",
    "            os.makedirs(out_sujeto_path)\n",
    "        for img in lfw[sujeto]:\n",
    "            src = os.path.join(lfw_path, sujeto, img)\n",
    "            dst = os.path.join(out_sujeto_path, \"lfw_\" + img)\n",
    "            shutil.copy2(src, dst)\n",
    "        for img in cplfw[sujeto]:\n",
    "            src = os.path.join(cplfw_path, img)\n",
    "            dst = os.path.join(out_sujeto_path, \"cplfw_\" + img)\n",
    "            shutil.copy2(src, dst)\n",
    "\n",
    "lfw, cplfw = merge_lfw_cplfw(LFW_PATH, CPLFW_PATH)\n",
    "sujetos_comunes = list(lfw.keys())\n",
    "print(f\"Sujetos en común antes del muestreo: {len(sujetos_comunes)}\")\n",
    "# Selección aleatoria reproducible\n",
    "random.seed(RANDOM_SEED)\n",
    "if len(sujetos_comunes) < N_SUJETOS:\n",
    "    print(f\"¡Advertencia! Solo hay {len(sujetos_comunes)} sujetos. Usando todos.\")\n",
    "    sujetos_seleccionados = sujetos_comunes\n",
    "else:\n",
    "    sujetos_seleccionados = random.sample(sujetos_comunes, N_SUJETOS)\n",
    "# Filtra los diccionarios para solo esos sujetos\n",
    "lfw = {k: lfw[k] for k in sujetos_seleccionados}\n",
    "cplfw = {k: cplfw[k] for k in sujetos_seleccionados}\n",
    "print(f\"Copiando sujetos seleccionados: {len(lfw)}\")\n",
    "copiar_imagenes_a_nueva_carpeta(lfw, cplfw, LFW_PATH, CPLFW_PATH, OUTPUT_PATH)\n",
    "print(f\"¡Listo! Las imágenes de 50 sujetos seleccionados se copiaron en {OUTPUT_PATH}\")"
   ],
   "id": "8b1502238ccfcd22",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sujetos en común antes del muestreo: 135\n",
      "Copiando sujetos seleccionados: 50\n",
      "¡Listo! Las imágenes de 50 sujetos seleccionados se copiaron en dataset/merged_lfw_cplfw_50\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from deepface import DeepFace\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Configuración\n",
    "DATASET_PATH = \"dataset/merged_lfw_cplfw\"\n",
    "SPLIT_SIZE = 10\n",
    "MODELS = [\"VGG-Face\", \"Facenet\", \"Facenet512\", \"OpenFace\", \"DeepFace\", \"ArcFace\", \"Dlib\", \"SFace\"]\n",
    "DETECTORS = [\"retinaface\", \"centerface\", \"yunet\", \"yolov8\"]\n",
    "\n",
    "def get_subjects_and_images(dataset_path):\n",
    "    \"\"\"\n",
    "    Devuelve un dict: sujeto -> {'lfw': [img1, ...], 'cplfw': [imgA, ...]}\n",
    "    \"\"\"\n",
    "    subjects = {}\n",
    "    for s in os.listdir(dataset_path):\n",
    "        subj_path = os.path.join(dataset_path, s)\n",
    "        if os.path.isdir(subj_path):\n",
    "            lfw_imgs = sorted([os.path.join(subj_path, f) for f in os.listdir(subj_path) if f.startswith(\"lfw_\")])\n",
    "            cplfw_imgs = sorted([os.path.join(subj_path, f) for f in os.listdir(subj_path) if f.startswith(\"cplfw_\")])\n",
    "            if len(lfw_imgs) == 10 and len(cplfw_imgs) > 0:\n",
    "                subjects[s] = {'lfw': lfw_imgs, 'cplfw': cplfw_imgs}\n",
    "    return subjects\n",
    "\n",
    "def split_subjects(subjects, split_size):\n",
    "    subject_list = list(subjects.keys())\n",
    "    return [subject_list[i:i+split_size] for i in range(0, len(subject_list), split_size)]\n",
    "\n",
    "def benchmark_block(model_name, detector, block_subjects, subjects):\n",
    "    references = {}  # sujeto: [img1, img2, ..., img10] (lfw)\n",
    "    probes = []\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    times = []\n",
    "\n",
    "    # Prepara referencias\n",
    "    for subj in block_subjects:\n",
    "        references[subj] = subjects[subj]['lfw']\n",
    "\n",
    "    # Prepara pruebas (todas las cplfw del bloque)\n",
    "    for subj in block_subjects:\n",
    "        for probe_img in subjects[subj]['cplfw']:\n",
    "            probes.append((probe_img, subj))\n",
    "            true_labels.append(subj)\n",
    "\n",
    "    # Inferencia\n",
    "    for probe_path, true_subj in probes:\n",
    "        start = time.time()\n",
    "        best_match = None\n",
    "        min_dist = float(\"inf\")\n",
    "        for ref_subj, ref_imgs in references.items():\n",
    "            # Compara con las 10 imágenes base, toma la mínima distancia\n",
    "            for ref_path in ref_imgs:\n",
    "                try:\n",
    "                    result = DeepFace.verify(\n",
    "                        img1_path=probe_path,\n",
    "                        img2_path=ref_path,\n",
    "                        model_name=model_name,\n",
    "                        detector_backend=detector,\n",
    "                        enforce_detection=(detector != \"skip\")\n",
    "                    )\n",
    "                    dist = result[\"distance\"]\n",
    "                    if dist < min_dist:\n",
    "                        min_dist = dist\n",
    "                        best_match = ref_subj\n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {probe_path} vs {ref_path} | {e}\")\n",
    "        pred_labels.append(best_match)\n",
    "        times.append(time.time() - start)\n",
    "\n",
    "    # Métricas\n",
    "    acc = accuracy_score(true_labels, pred_labels)\n",
    "    prec = precision_score(true_labels, pred_labels, average=\"macro\", zero_division=0)\n",
    "    recall = recall_score(true_labels, pred_labels, average=\"macro\", zero_division=0)\n",
    "    f1 = f1_score(true_labels, pred_labels, average=\"macro\", zero_division=0)\n",
    "    avg_time = np.mean(times)\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"detector\": detector,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"avg_time\": avg_time,\n",
    "        \"n_tests\": len(probes)\n",
    "    }\n",
    "\n",
    "def benchmark_init():\n",
    "    subjects = get_subjects_and_images(DATASET_PATH)\n",
    "    splits = split_subjects(subjects, SPLIT_SIZE)\n",
    "    results = []\n",
    "    for model in MODELS:\n",
    "        for detector in DETECTORS:\n",
    "            print(f\"Evaluando modelo: {model} | detector: {detector}\")\n",
    "            for i, block_subjects in enumerate(splits):\n",
    "                print(f\" - Split {i+1}/{len(splits)} ({len(block_subjects)} sujetos)\")\n",
    "                metrics = benchmark_block(model, detector, block_subjects, subjects)\n",
    "                metrics[\"split\"] = i+1\n",
    "                results.append(metrics)\n",
    "                print(metrics)\n",
    "    pd.DataFrame(results).to_csv(\"benchmark_results_detectors.csv\", index=False)"
   ],
   "id": "3e94583b18c4a91a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "benchmark_init()",
   "id": "b1829e180651067"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
