{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-20T21:17:36.472905Z",
     "start_time": "2025-04-20T21:17:29.599782Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import MultiHeadAttention, GlobalAveragePooling1D"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T21:18:35.346255Z",
     "start_time": "2025-04-20T21:18:35.325199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_fall_detection_model_multiattn(timesteps, features, lstm_units=64, dropout_rate=0.2,\n",
    "                                         dense_units=34, l2_reg=0.01, num_classes=2,\n",
    "                                         num_heads=4, key_dim=16):\n",
    "    \"\"\"\n",
    "    Construye un modelo RNN con MultiHeadAttention y GlobalAveragePooling1D.\n",
    "\n",
    "    Args:\n",
    "        timesteps (int): Número de pasos de tiempo en cada secuencia.\n",
    "        features (int): Número de características por paso de tiempo.\n",
    "        lstm_units (int): Unidades de la LSTM.\n",
    "        dropout_rate (float): Tasa de dropout.\n",
    "        dense_units (int): Unidades en la capa densa intermedia.\n",
    "        l2_reg (float): Regularización L2.\n",
    "        num_classes (int): Número de clases de salida.\n",
    "        num_heads (int): Número de \"cabezas\" de atención.\n",
    "        key_dim (int): Dimensión de las claves por cabeza.\n",
    "\n",
    "    Returns:\n",
    "        keras.Model: Modelo compilado.\n",
    "    \"\"\"\n",
    "    input_shape = (timesteps, features)\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # 1. Masking\n",
    "    masked_inputs = layers.Masking(mask_value=0.0)(inputs)\n",
    "\n",
    "    # 2. LSTM\n",
    "    lstm_out = layers.LSTM(lstm_units, return_sequences=True)(masked_inputs)\n",
    "\n",
    "    # 3. Dropout\n",
    "    dropout_out = layers.Dropout(dropout_rate)(lstm_out)\n",
    "\n",
    "    # 4. MultiHead Attention\n",
    "    attn_out = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(dropout_out, dropout_out)\n",
    "\n",
    "    # 5. GlobalAveragePooling\n",
    "    pooled_out = GlobalAveragePooling1D()(attn_out)\n",
    "\n",
    "    # 6. Dense\n",
    "    dense_out = layers.Dense(dense_units, activation='relu',\n",
    "                             kernel_initializer=RandomNormal(),\n",
    "                             kernel_regularizer=l2(l2_reg))(pooled_out)\n",
    "\n",
    "    # 7. Output\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(dense_out)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val=None, y_val=None,\n",
    "                epochs=50, batch_size=32, validation_split=0.2,\n",
    "                patience=10):\n",
    "    \"\"\"\n",
    "    Compila y entrena el modelo Keras.\n",
    "    (Misma función que antes)\n",
    "    \"\"\"\n",
    "    # Compilar el modelo\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy', # Confirmado que 'y' es one-hot (2 cols)\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Callback para detener el entrenamiento si no mejora\n",
    "    early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                   patience=patience,\n",
    "                                   restore_best_weights=True)\n",
    "\n",
    "    # Determinar datos de validación\n",
    "    if X_val is not None and y_val is not None:\n",
    "        validation_data = (X_val, y_val)\n",
    "        val_split = 0.0\n",
    "    else:\n",
    "        validation_data = None\n",
    "        val_split = validation_split\n",
    "\n",
    "    print(f\"\\nIniciando entrenamiento por {epochs} épocas...\")\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_split=val_split,\n",
    "                        validation_data=validation_data,\n",
    "                        callbacks=[early_stopping],\n",
    "                        verbose=1)\n",
    "\n",
    "    print(\"Entrenamiento completado.\")\n",
    "    return history"
   ],
   "id": "ca5a3a6bccd6980f",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T21:31:31.890094Z",
     "start_time": "2025-04-20T21:31:24.032926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('combined_padded.csv')\n",
    "\n",
    "# Asegúrate de que los campos 'features' y 'poses_sec' son listas\n",
    "def safe_literal_eval(val):\n",
    "    # Maneja valores nulos/faltantes devolviendo lista vacía o None según prefieras\n",
    "    if pd.isna(val):\n",
    "        return [] # O manejar de otra forma si es necesario\n",
    "    if isinstance(val, str):\n",
    "        try:\n",
    "            return ast.literal_eval(val)\n",
    "        except (ValueError, SyntaxError):\n",
    "            print(f\"Advertencia: No se pudo evaluar: {val}\")\n",
    "            return [] # O manejar de otra forma\n",
    "    return val # Si ya es una lista/otro tipo\n",
    "\n",
    "df['features'] = df['features'].apply(safe_literal_eval)\n",
    "df['poses_sec'] = df['poses_sec'].apply(safe_literal_eval)\n",
    "\n",
    "# -------------------------\n",
    "# 2. Combinar features + landmarks por frame\n",
    "# -------------------------\n",
    "sequences = []\n",
    "max_len_actual = 0 # Para verificar la longitud si es necesario\n",
    "for i, (feats, lms) in enumerate(zip(df['features'], df['poses_sec'])):\n",
    "    # Añadir una verificación por si una de las listas está vacía\n",
    "    if not feats or not lms:\n",
    "        print(f\"Advertencia: Lista vacía encontrada en índice {i}. Saltando muestra.\")\n",
    "        continue # O manejar añadiendo ceros si es apropiado\n",
    "\n",
    "    # Verifica si las longitudes coinciden antes de hacer zip\n",
    "    if len(feats) != len(lms):\n",
    "        print(f\"Advertencia: Longitudes no coinciden en índice {i}. feats: {len(feats)}, lms: {len(lms)}. Usando la longitud menor.\")\n",
    "        min_len = min(len(feats), len(lms))\n",
    "        feats = feats[:min_len]\n",
    "        lms = lms[:min_len]\n",
    "\n",
    "    combined_seq = [f + l for f, l in zip(feats, lms)]\n",
    "    #print(combined_seq)\n",
    "    if not combined_seq: # Si después del zip sigue vacío\n",
    "        print(f\"Advertencia: Secuencia combinada vacía en índice {i}. Saltando.\")\n",
    "        continue\n",
    "    sequences.append(combined_seq)\n",
    "    max_len_actual = max(max_len_actual, len(combined_seq))\n",
    "\n",
    "# Asegúrate de que todas las secuencias tengan la misma longitud (Padding ya debería estar hecho)\n",
    "# Si el padding no se hizo antes o fue inconsistente, aquí es donde fallaría np.array()\n",
    "print(f\"Longitud máxima encontrada en secuencias combinadas: {max_len_actual}\") \n",
    "# Debería ser 129 según tus datos originales.\n",
    "\n",
    "# Convertir a array NumPy. Si falla aquí, el padding previo tuvo problemas.\n",
    "try:\n",
    "    X = np.array(sequences, dtype=np.float32) # Especificar dtype puede ayudar\n",
    "except ValueError as e:\n",
    "    print(f\"Error al convertir a NumPy array: {e}\")\n",
    "    print(\"Esto usualmente indica que las secuencias internas tienen longitudes diferentes.\")\n",
    "    # Aquí podrías añadir código para forzar el padding si es necesario,\n",
    "    # pero basado en tu archivo 'combined_padded.csv', no debería ser necesario.\n",
    "    # Ejemplo de padding si fuera necesario:\n",
    "    # from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    # X = pad_sequences(sequences, maxlen=129, padding='post', truncating='post', dtype='float32')\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 3. Preparar las etiquetas\n",
    "# -------------------------\n",
    "y_labels = df['target'].values # Asegúrate que la columna se llama 'target'\n",
    "encoder = LabelBinarizer()\n",
    "y = encoder.fit_transform(y_labels)\n",
    "\n",
    "if y.shape[1] == 1:\n",
    "    y = np.hstack((1 - y, y))\n",
    "\n",
    "# Eliminar filas de 'y' donde se saltaron secuencias en 'X' (si se hizo 'continue')\n",
    "# Esto requiere rastrear los índices saltados o re-filtrar df antes de obtener y_labels\n",
    "# Por simplicidad, asumimos que no se saltaron filas por ahora.\n",
    "\n",
    "# -------------------------\n",
    "# 4. Separar en train/test\n",
    "# -------------------------\n",
    "# Asegúrate que X y y tengan el mismo número de muestras antes de dividir\n",
    "if X.shape[0] != y.shape[0]:\n",
    "    raise ValueError(f\"Discrepancia en número de muestras: X tiene {X.shape[0]}, y tiene {y.shape[0]}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) # Añadir stratify es bueno para clasificación\n",
    "\n",
    "print(\"Formas finales de los datos:\")\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ],
   "id": "51dfef8047dbc57c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud máxima encontrada en secuencias combinadas: 130\n",
      "Formas finales de los datos:\n",
      "(118, 130, 109) (30, 130, 109) (118, 2) (30, 2)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T21:32:36.672465Z",
     "start_time": "2025-04-20T21:32:22.574204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if X_train.size == 0:\n",
    "    print(\"Error: X_train está vacío después del preprocesamiento.\")\n",
    "else:\n",
    "    num_samples, timesteps, features = X_train.shape\n",
    "    num_classes = y_train.shape[1]\n",
    "\n",
    "    # Construir el modelo con Attention\n",
    "    fall_model_attn = build_fall_detection_model_multiattn(timesteps=timesteps,\n",
    "                                                                features=features,\n",
    "                                                                num_classes=num_classes)\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    history = train_model(fall_model_attn, X_train, y_train, epochs=100, batch_size=16, validation_split=0.1)\n",
    "\n",
    "    # Evaluar después\n",
    "    print(\"\\nEvaluando el modelo (con Attention) en el conjunto de prueba:\")\n",
    "    loss, accuracy = fall_model_attn.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Pérdida en Prueba: {loss:.4f}\")\n",
    "    print(f\"Precisión en Prueba: {accuracy:.4f}\")"
   ],
   "id": "bfd777f2e94d4d46",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando entrenamiento por 100 épocas...\n",
      "Epoch 1/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 97ms/step - accuracy: 0.3894 - loss: 0.7488 - val_accuracy: 0.7333 - val_loss: 0.7287\n",
      "Epoch 2/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 44ms/step - accuracy: 0.5044 - loss: 0.7308 - val_accuracy: 0.5333 - val_loss: 0.7125\n",
      "Epoch 3/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 44ms/step - accuracy: 0.5002 - loss: 0.7240 - val_accuracy: 0.5333 - val_loss: 0.6901\n",
      "Epoch 4/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 46ms/step - accuracy: 0.5255 - loss: 0.7059 - val_accuracy: 0.6333 - val_loss: 0.6490\n",
      "Epoch 5/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 42ms/step - accuracy: 0.6164 - loss: 0.6704 - val_accuracy: 0.8000 - val_loss: 0.5868\n",
      "Epoch 6/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 44ms/step - accuracy: 0.7209 - loss: 0.6150 - val_accuracy: 0.7667 - val_loss: 0.5199\n",
      "Epoch 7/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 44ms/step - accuracy: 0.6816 - loss: 0.5969 - val_accuracy: 0.8000 - val_loss: 0.4662\n",
      "Epoch 8/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 46ms/step - accuracy: 0.7856 - loss: 0.5190 - val_accuracy: 0.8000 - val_loss: 0.4495\n",
      "Epoch 9/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 45ms/step - accuracy: 0.7946 - loss: 0.4527 - val_accuracy: 0.8000 - val_loss: 0.5195\n",
      "Epoch 10/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 45ms/step - accuracy: 0.8089 - loss: 0.4689 - val_accuracy: 0.8333 - val_loss: 0.4331\n",
      "Epoch 11/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 48ms/step - accuracy: 0.8562 - loss: 0.4174 - val_accuracy: 0.8333 - val_loss: 0.4598\n",
      "Epoch 12/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 46ms/step - accuracy: 0.8596 - loss: 0.3910 - val_accuracy: 0.8667 - val_loss: 0.4064\n",
      "Epoch 13/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 47ms/step - accuracy: 0.8332 - loss: 0.3841 - val_accuracy: 0.8333 - val_loss: 0.4577\n",
      "Epoch 14/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 46ms/step - accuracy: 0.8745 - loss: 0.3694 - val_accuracy: 0.8000 - val_loss: 0.4779\n",
      "Epoch 15/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 45ms/step - accuracy: 0.8684 - loss: 0.3397 - val_accuracy: 0.9000 - val_loss: 0.4067\n",
      "Epoch 16/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 46ms/step - accuracy: 0.9111 - loss: 0.2731 - val_accuracy: 0.8333 - val_loss: 0.4869\n",
      "Epoch 17/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 45ms/step - accuracy: 0.8906 - loss: 0.2825 - val_accuracy: 0.8333 - val_loss: 0.3449\n",
      "Epoch 18/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 45ms/step - accuracy: 0.8547 - loss: 0.3230 - val_accuracy: 0.8000 - val_loss: 0.4561\n",
      "Epoch 19/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 44ms/step - accuracy: 0.9454 - loss: 0.1707 - val_accuracy: 0.8667 - val_loss: 0.3465\n",
      "Epoch 20/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 42ms/step - accuracy: 0.9347 - loss: 0.1720 - val_accuracy: 0.9000 - val_loss: 0.3876\n",
      "Epoch 21/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 46ms/step - accuracy: 0.9850 - loss: 0.1019 - val_accuracy: 0.8333 - val_loss: 0.4063\n",
      "Epoch 22/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 44ms/step - accuracy: 0.9613 - loss: 0.0986 - val_accuracy: 0.8000 - val_loss: 0.6578\n",
      "Epoch 23/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 44ms/step - accuracy: 0.9772 - loss: 0.0819 - val_accuracy: 0.8333 - val_loss: 0.4288\n",
      "Epoch 24/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 44ms/step - accuracy: 0.9871 - loss: 0.0619 - val_accuracy: 0.8333 - val_loss: 0.5620\n",
      "Epoch 25/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 45ms/step - accuracy: 1.0000 - loss: 0.0400 - val_accuracy: 0.8333 - val_loss: 0.6442\n",
      "Epoch 26/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 44ms/step - accuracy: 1.0000 - loss: 0.0359 - val_accuracy: 0.8333 - val_loss: 0.6154\n",
      "Epoch 27/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 44ms/step - accuracy: 1.0000 - loss: 0.0371 - val_accuracy: 0.8000 - val_loss: 0.9964\n",
      "Entrenamiento completado.\n",
      "\n",
      "Evaluando el modelo (con Attention) en el conjunto de prueba:\n",
      "Pérdida en Prueba: 0.3449\n",
      "Precisión en Prueba: 0.8333\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T21:34:55.083769Z",
     "start_time": "2025-04-20T21:34:54.992344Z"
    }
   },
   "cell_type": "code",
   "source": "fall_model_attn.save('fall_detection_model.keras')",
   "id": "c83ef3c7b0e79c4",
   "outputs": [],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
