{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0ab0940",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5039aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"C:/Users/Invitado/Documents/fall_detection/runs/detect/train7/weights/best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff722b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 448x640 1 person, 188.0ms\n",
      "Speed: 2.2ms preprocess, 188.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x416 (no detections), 14.8ms\n",
      "Speed: 1.5ms preprocess, 14.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 448x640 1 person, 45.2ms\n",
      "Speed: 1.2ms preprocess, 45.2ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x320 (no detections), 9.0ms\n",
      "Speed: 0.8ms preprocess, 9.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 320)\n",
      "\n",
      "0: 448x640 1 person, 49.5ms\n",
      "Speed: 1.1ms preprocess, 49.5ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x320 (no detections), 7.2ms\n",
      "Speed: 0.8ms preprocess, 7.2ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 320)\n",
      "\n",
      "0: 448x640 1 person, 44.1ms\n",
      "Speed: 1.3ms preprocess, 44.1ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x288 (no detections), 8.1ms\n",
      "Speed: 0.7ms preprocess, 8.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 288)\n",
      "\n",
      "0: 448x640 1 person, 51.1ms\n",
      "Speed: 1.2ms preprocess, 51.1ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x288 (no detections), 7.4ms\n",
      "Speed: 1.0ms preprocess, 7.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 288)\n",
      "\n",
      "0: 448x640 1 person, 45.5ms\n",
      "Speed: 1.1ms preprocess, 45.5ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x256 (no detections), 7.5ms\n",
      "Speed: 1.1ms preprocess, 7.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 256)\n",
      "\n",
      "0: 448x640 1 person, 45.8ms\n",
      "Speed: 1.1ms preprocess, 45.8ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x256 (no detections), 7.3ms\n",
      "Speed: 0.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 256)\n",
      "\n",
      "0: 448x640 1 person, 46.4ms\n",
      "Speed: 1.3ms preprocess, 46.4ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x256 (no detections), 7.3ms\n",
      "Speed: 0.9ms preprocess, 7.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 256)\n",
      "\n",
      "0: 448x640 1 person, 46.2ms\n",
      "Speed: 1.5ms preprocess, 46.2ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x224 (no detections), 7.9ms\n",
      "Speed: 0.9ms preprocess, 7.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 224)\n",
      "\n",
      "0: 448x640 1 person, 50.9ms\n",
      "Speed: 1.2ms preprocess, 50.9ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x224 (no detections), 6.9ms\n",
      "Speed: 0.8ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 224)\n",
      "\n",
      "0: 448x640 1 person, 46.1ms\n",
      "Speed: 1.5ms preprocess, 46.1ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x224 (no detections), 6.9ms\n",
      "Speed: 0.7ms preprocess, 6.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 224)\n",
      "\n",
      "0: 448x640 1 person, 51.2ms\n",
      "Speed: 1.3ms preprocess, 51.2ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x256 (no detections), 7.6ms\n",
      "Speed: 0.8ms preprocess, 7.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 256)\n",
      "\n",
      "0: 448x640 1 person, 43.3ms\n",
      "Speed: 1.1ms preprocess, 43.3ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x256 (no detections), 7.2ms\n",
      "Speed: 0.8ms preprocess, 7.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 256)\n",
      "\n",
      "0: 448x640 1 person, 45.1ms\n",
      "Speed: 1.1ms preprocess, 45.1ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x256 (no detections), 7.1ms\n",
      "Speed: 0.7ms preprocess, 7.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 256)\n",
      "\n",
      "0: 448x640 1 person, 44.6ms\n",
      "Speed: 1.2ms preprocess, 44.6ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x256 (no detections), 6.8ms\n",
      "Speed: 0.8ms preprocess, 6.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 256)\n",
      "\n",
      "0: 448x640 1 person, 45.1ms\n",
      "Speed: 1.4ms preprocess, 45.1ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x320 (no detections), 7.3ms\n",
      "Speed: 0.9ms preprocess, 7.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 320)\n",
      "\n",
      "0: 448x640 1 person, 48.9ms\n",
      "Speed: 1.1ms preprocess, 48.9ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x320 (no detections), 6.8ms\n",
      "Speed: 0.8ms preprocess, 6.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 320)\n",
      "\n",
      "0: 448x640 1 person, 51.2ms\n",
      "Speed: 1.2ms preprocess, 51.2ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x384 1 Fall, 7.5ms\n",
      "Speed: 1.0ms preprocess, 7.5ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 448x640 1 person, 55.7ms\n",
      "Speed: 1.3ms preprocess, 55.7ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x480 (no detections), 7.8ms\n",
      "Speed: 1.0ms preprocess, 7.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 448x640 1 person, 43.2ms\n",
      "Speed: 1.1ms preprocess, 43.2ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x416 (no detections), 7.6ms\n",
      "Speed: 1.0ms preprocess, 7.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 448x640 1 person, 45.9ms\n",
      "Speed: 1.1ms preprocess, 45.9ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x512 (no detections), 7.5ms\n",
      "Speed: 1.0ms preprocess, 7.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "0: 448x640 1 person, 54.2ms\n",
      "Speed: 1.3ms preprocess, 54.2ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x544 1 Fall, 7.8ms\n",
      "Speed: 1.1ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 448x640 1 person, 44.8ms\n",
      "Speed: 1.2ms preprocess, 44.8ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.8ms\n",
      "Speed: 1.6ms preprocess, 7.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 55.9ms\n",
      "Speed: 1.2ms preprocess, 55.9ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 2 Falls, 7.7ms\n",
      "Speed: 0.9ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 1 person, 86.2ms\n",
      "Speed: 1.2ms preprocess, 86.2ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 352x640 2 Falls, 8.1ms\n",
      "Speed: 1.0ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 448x640 1 person, 176.9ms\n",
      "Speed: 1.4ms preprocess, 176.9ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 352x640 1 Fall, 14.2ms\n",
      "Speed: 1.5ms preprocess, 14.2ms inference, 2.6ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 448x640 1 person, 83.0ms\n",
      "Speed: 1.5ms preprocess, 83.0ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 352x640 2 Falls, 7.1ms\n",
      "Speed: 0.8ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 448x640 1 person, 56.7ms\n",
      "Speed: 1.2ms preprocess, 56.7ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 352x640 2 Falls, 7.5ms\n",
      "Speed: 0.8ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 448x640 1 person, 151.6ms\n",
      "Speed: 1.2ms preprocess, 151.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 14.9ms\n",
      "Speed: 1.5ms preprocess, 14.9ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 55.0ms\n",
      "Speed: 1.9ms preprocess, 55.0ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 1 Fall, 8.0ms\n",
      "Speed: 0.9ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 1 person, 93.1ms\n",
      "Speed: 1.4ms preprocess, 93.1ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 1 Fall, 13.9ms\n",
      "Speed: 1.4ms preprocess, 13.9ms inference, 2.7ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 1 person, 53.8ms\n",
      "Speed: 1.4ms preprocess, 53.8ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 Fall, 10.3ms\n",
      "Speed: 1.0ms preprocess, 10.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 141.5ms\n",
      "Speed: 1.2ms preprocess, 141.5ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 1 Fall, 14.7ms\n",
      "Speed: 1.9ms preprocess, 14.7ms inference, 2.7ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 1 person, 190.3ms\n",
      "Speed: 1.3ms preprocess, 190.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 1 Fall, 14.8ms\n",
      "Speed: 1.5ms preprocess, 14.8ms inference, 2.7ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 1 person, 55.9ms\n",
      "Speed: 1.9ms preprocess, 55.9ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 1 Fall, 8.1ms\n",
      "Speed: 1.1ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 1 person, 171.2ms\n",
      "Speed: 1.2ms preprocess, 171.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 1 Fall, 15.4ms\n",
      "Speed: 1.6ms preprocess, 15.4ms inference, 2.9ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 1 person, 171.7ms\n",
      "Speed: 1.2ms preprocess, 171.7ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 1 Fall, 13.3ms\n",
      "Speed: 1.9ms preprocess, 13.3ms inference, 2.7ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 1 person, 135.3ms\n",
      "Speed: 1.5ms preprocess, 135.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 2 Falls, 13.3ms\n",
      "Speed: 2.2ms preprocess, 13.3ms inference, 2.7ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 1 person, 83.5ms\n",
      "Speed: 1.4ms preprocess, 83.5ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 2 Falls, 10.3ms\n",
      "Speed: 1.4ms preprocess, 10.3ms inference, 2.6ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 1 person, 86.5ms\n",
      "Speed: 1.4ms preprocess, 86.5ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 Falls, 11.7ms\n",
      "Speed: 1.5ms preprocess, 11.7ms inference, 2.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 90.1ms\n",
      "Speed: 1.2ms preprocess, 90.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 Fall, 13.3ms\n",
      "Speed: 2.0ms preprocess, 13.3ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 54.8ms\n",
      "Speed: 1.4ms preprocess, 54.8ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 2 Falls, 11.3ms\n",
      "Speed: 1.4ms preprocess, 11.3ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(\"C:/Users/Invitado/Downloads/FallBackwardsS1.avi\")\n",
    "video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_step = int(video_fps // 6)\n",
    "frame_idx = 0\n",
    "model_yolo = YOLO(\"yolo11n.pt\")\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "    if frame_idx % frame_step == 0:\n",
    "        # Run YOLO inference on the frame\n",
    "        results = model_yolo(frame, classes=[0], conf = 0.5, device=\"cpu\")\n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])  # Bounding box\n",
    "                cropped = frame[y1:y2, x1:x2]  # Recortar persona\n",
    "\n",
    "                if cropped.size == 0:\n",
    "                    continue\n",
    "\n",
    "                fall_res = model(cropped, classes=[1], conf = 0.5, device=\"cpu\")\n",
    "                fall_detected = any(r.boxes for r in fall_res)\n",
    "\n",
    "                label = \"FALL\" if fall_detected else \"Normal\"\n",
    "\n",
    "                # Dibujar la caja y el label\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255) if fall_detected else (0, 255, 0), 2)\n",
    "                cv2.putText(frame, label, (x1, y1 - 40), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            0.9, (0, 0, 255) if fall_detected else (255, 255, 255), 2)\n",
    "        frame_idx = 0\n",
    "    frame_idx += 1\n",
    "\n",
    "    # Visualize the results on the frame\n",
    "    frame = results[0].plot()\n",
    "    # Display the annotated frame\n",
    "    cv2.imshow(\"YOLO Inference\", frame)\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ca66048",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m cap = \u001b[43mcv2\u001b[49m.VideoCapture(\u001b[33m\"\u001b[39m\u001b[33mC:/Users/Invitado/Documents/fall_detection/data/GMDCSA24/Subject 4/Fall/08.mp4\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m model_yolo = YOLO(\u001b[33m\"\u001b[39m\u001b[33myolo11n.pt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m cap.isOpened():\n",
      "\u001b[31mNameError\u001b[39m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(\"C:/Users/Invitado/Documents/fall_detection/data/GMDCSA24/Subject 4/Fall/08.mp4\")\n",
    "model_yolo = YOLO(\"yolo11n.pt\")\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "        # Run YOLO inference on the frame\n",
    "    results = model_yolo(frame, classes=[0], conf = 0.5)\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])  # Bounding box\n",
    "            cropped = frame[y1:y2, x1:x2]  # Recortar persona\n",
    "\n",
    "            if cropped.size == 0:\n",
    "                continue\n",
    "\n",
    "            fall_res = model(cropped, classes=[1], conf = 0.5)\n",
    "            fall_detected = any(r.boxes for r in fall_res)\n",
    "\n",
    "            label = \"FALL\" if fall_detected else \"Normal\"\n",
    "\n",
    "            # Dibujar la caja y el label\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255) if fall_detected else (0, 255, 0), 2)\n",
    "            cv2.putText(frame, label, (x1, y1 - 40), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.9, (0, 0, 255) if fall_detected else (255, 255, 255), 2)\n",
    "\n",
    "    # Visualize the results on the frame\n",
    "    frame = results[0].plot()\n",
    "    # Display the annotated frame\n",
    "    cv2.imshow(\"YOLO Inference\", frame)\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e92fdc5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_yolo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m frame = cv2.imread(\u001b[33m\"\u001b[39m\u001b[33mC:/Users/Invitado/Documents/fall_detection/data/URFD/Fall/Fall01/Camera/fall-01-cam0-rgb-110.png\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m results = \u001b[43mmodel_yolo\u001b[49m(frame, classes=[\u001b[32m0\u001b[39m])\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m box \u001b[38;5;129;01min\u001b[39;00m result.boxes:\n",
      "\u001b[31mNameError\u001b[39m: name 'model_yolo' is not defined"
     ]
    }
   ],
   "source": [
    "frame = cv2.imread(\"C:/Users/Invitado/Documents/fall_detection/data/URFD/Fall/Fall01/Camera/fall-01-cam0-rgb-110.png\")\n",
    "results = model_yolo(frame, classes=[0])\n",
    "for result in results:\n",
    "    for box in result.boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])  # Bounding box\n",
    "        cropped = frame[y1:y2, x1:x2]  # Recortar persona\n",
    "\n",
    "        if cropped.size == 0:\n",
    "            continue\n",
    "\n",
    "        fall_res = model(cropped, classes=[1])\n",
    "        fall_detected = any(r.boxes for r in fall_res)\n",
    "\n",
    "        label = \"FALL\" if fall_detected else \"Normal\"\n",
    "\n",
    "        # Dibujar la caja y el label\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255) if fall_detected else (0, 255, 0), 2)\n",
    "        cv2.putText(frame, label, (x1, y1 - 40), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.9, (0, 0, 255) if fall_detected else (255, 255, 255), 2)\n",
    "\n",
    "# Visualize the results on the frame\n",
    "frame = results[0].plot()\n",
    "# Display the annotated frame\n",
    "plt.imshow(frame)\n",
    "# Break the loop if 'q' is pressed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
