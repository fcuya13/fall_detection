{
 "cells": [
  {
   "cell_type": "code",
   "id": "d0ab0940",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T17:36:08.863004Z",
     "start_time": "2025-05-04T17:36:08.844188Z"
    }
   },
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "c5039aca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T17:36:13.970898Z",
     "start_time": "2025-05-04T17:36:10.349089Z"
    }
   },
   "source": "model = YOLO(\"C:/Users/bryan/OneDrive/Documentos/Tesis/fall_detection/runs/classification/best.pt\")",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T17:44:54.754181Z",
     "start_time": "2025-05-04T17:44:54.497798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "res = model('dataset/Merged_Dataset_for_classification/test/0/CAUCAFall_ars100017_0.png')\n",
    "print(res[0].probs.top1)\n",
    "for re in  res:\n",
    "    print(\"----------------------------------\")\n",
    "    print(re)"
   ],
   "id": "2cec51fef72e4915",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\bryan\\OneDrive\\Documentos\\Tesis\\fall_detection\\yolo\\dataset\\Merged_Dataset_for_classification\\test\\0\\CAUCAFall_ars100017_0.png: 224x224 ADL 1.00, Fall 0.00, 25.4ms\n",
      "Speed: 190.1ms preprocess, 25.4ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "0\n",
      "----------------------------------\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: None\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'ADL', 1: 'Fall'}\n",
      "obb: None\n",
      "orig_img: array([[[  5,  10,  23],\n",
      "        [  4,   9,  22],\n",
      "        [  2,   7,  20],\n",
      "        ...,\n",
      "        [ 34,  48,  75],\n",
      "        [ 48,  61,  89],\n",
      "        [ 64,  78, 107]],\n",
      "\n",
      "       [[  8,  13,  26],\n",
      "        [  5,  10,  23],\n",
      "        [  2,   7,  20],\n",
      "        ...,\n",
      "        [ 35,  50,  77],\n",
      "        [ 46,  60,  88],\n",
      "        [ 68,  83, 114]],\n",
      "\n",
      "       [[ 11,  15,  28],\n",
      "        [  6,  11,  24],\n",
      "        [  3,   7,  21],\n",
      "        ...,\n",
      "        [ 36,  52,  79],\n",
      "        [ 43,  59,  87],\n",
      "        [ 68,  84, 115]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[161, 165, 178],\n",
      "        [139, 144, 157],\n",
      "        [104, 108, 123],\n",
      "        ...,\n",
      "        [ 71,  74,  86],\n",
      "        [ 73,  76,  89],\n",
      "        [ 79,  81,  95]],\n",
      "\n",
      "       [[163, 168, 179],\n",
      "        [167, 173, 183],\n",
      "        [177, 183, 193],\n",
      "        ...,\n",
      "        [ 80,  82,  96],\n",
      "        [ 83,  84, 101],\n",
      "        [ 79,  80,  98]],\n",
      "\n",
      "       [[154, 161, 169],\n",
      "        [155, 162, 170],\n",
      "        [159, 165, 174],\n",
      "        ...,\n",
      "        [ 81,  84, 100],\n",
      "        [ 84,  84, 105],\n",
      "        [ 82,  84, 104]]], dtype=uint8)\n",
      "orig_shape: (282, 113)\n",
      "path: 'C:\\\\Users\\\\bryan\\\\OneDrive\\\\Documentos\\\\Tesis\\\\fall_detection\\\\yolo\\\\dataset\\\\Merged_Dataset_for_classification\\\\test\\\\0\\\\CAUCAFall_ars100017_0.png'\n",
      "probs: ultralytics.engine.results.Probs object\n",
      "save_dir: 'runs\\\\classify\\\\predict'\n",
      "speed: {'preprocess': 190.06870000157505, 'inference': 25.433799979509786, 'postprocess': 0.08719999459572136}\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "ff722b11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T17:49:35.579077Z",
     "start_time": "2025-05-04T17:49:30.926762Z"
    }
   },
   "source": [
    "cap = cv2.VideoCapture(\"C:/Users/bryan/OneDrive/Documentos/Tesis/GMASD/GMDCSA24-A-Dataset-for-Human-Fall-Detection-in-Videos/data/Subject 1/Fall/06.mp4\")\n",
    "video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_rate = 6\n",
    "frame_step = int(video_fps // frame_rate)\n",
    "frame_idx = 0\n",
    "model_yolo = YOLO(\"yolo11n.pt\")\n",
    "fall_counter = 0  # Contador de frames con caída\n",
    "fall_threshold = 6  # Número de frames consecutivos necesarios\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    if frame_idx % frame_step == 0:\n",
    "        results = model_yolo(frame, classes=[0], conf=0.4, device=\"cpu\", stream=True)\n",
    "        fall_detected_in_frame = False  # Bandera para este frame\n",
    "\n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                cropped = frame[y1:y2, x1:x2]\n",
    "\n",
    "                if cropped.size == 0:\n",
    "                    continue\n",
    "\n",
    "                fall_res = model(cropped, classes=[1], conf=0.5, device=\"cpu\")\n",
    "                fall_detected = True if fall_res[0].probs.top1 == 1 else False\n",
    "\n",
    "                if fall_detected:\n",
    "                    fall_detected_in_frame = True\n",
    "\n",
    "                label = \"FALL\" if fall_detected else \"Normal\"\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255) if fall_detected else (0, 255, 0), 2)\n",
    "                cv2.putText(frame, label, (x1, y1), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            0.9, (0, 0, 255) if fall_detected else (255, 255, 255), 2)\n",
    "\n",
    "        # Actualizar contador de caídas consecutivas\n",
    "        if fall_detected_in_frame:\n",
    "            fall_counter += 1\n",
    "        else:\n",
    "            fall_counter = 0\n",
    "\n",
    "        if fall_counter >= fall_threshold:\n",
    "            print(\"-------------------- CAIDA DETECTADA -------------------- \")\n",
    "            fall_counter = 0  # Reiniciar contador después de detección\n",
    "\n",
    "        frame_idx = 0\n",
    "    frame_idx += 1\n",
    "\n",
    "    cv2.imshow(\"YOLO Inference\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 59.4ms\n",
      "\n",
      "0: 224x224 ADL 0.99, Fall 0.01, 24.6ms\n",
      "Speed: 5.4ms preprocess, 24.6ms inference, 0.2ms postprocess per image at shape (1, 3, 224, 224)\n",
      "Speed: 2.4ms preprocess, 59.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 76.7ms\n",
      "\n",
      "0: 224x224 ADL 1.00, Fall 0.00, 29.1ms\n",
      "Speed: 8.7ms preprocess, 29.1ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "Speed: 4.2ms preprocess, 76.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 63.4ms\n",
      "\n",
      "0: 224x224 ADL 1.00, Fall 0.00, 26.7ms\n",
      "Speed: 5.1ms preprocess, 26.7ms inference, 0.2ms postprocess per image at shape (1, 3, 224, 224)\n",
      "Speed: 2.9ms preprocess, 63.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 65.8ms\n",
      "\n",
      "0: 224x224 ADL 0.93, Fall 0.07, 26.7ms\n",
      "Speed: 5.1ms preprocess, 26.7ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "Speed: 3.8ms preprocess, 65.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 54.8ms\n",
      "\n",
      "0: 224x224 ADL 0.55, Fall 0.45, 25.9ms\n",
      "Speed: 5.5ms preprocess, 25.9ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "Speed: 3.9ms preprocess, 54.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 67.2ms\n",
      "\n",
      "0: 224x224 ADL 0.95, Fall 0.05, 26.0ms\n",
      "Speed: 6.5ms preprocess, 26.0ms inference, 0.2ms postprocess per image at shape (1, 3, 224, 224)\n",
      "Speed: 3.3ms preprocess, 67.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.0ms\n",
      "\n",
      "0: 224x224 ADL 0.99, Fall 0.01, 26.4ms\n",
      "Speed: 5.5ms preprocess, 26.4ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "Speed: 4.4ms preprocess, 72.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 61.0ms\n",
      "\n",
      "0: 224x224 ADL 0.99, Fall 0.01, 27.6ms\n",
      "Speed: 4.8ms preprocess, 27.6ms inference, 0.2ms postprocess per image at shape (1, 3, 224, 224)\n",
      "Speed: 4.2ms preprocess, 61.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 57.5ms\n",
      "\n",
      "0: 224x224 ADL 0.99, Fall 0.01, 26.5ms\n",
      "Speed: 5.1ms preprocess, 26.5ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "Speed: 3.4ms preprocess, 57.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 57.1ms\n",
      "\n",
      "0: 224x224 ADL 0.99, Fall 0.01, 25.5ms\n",
      "Speed: 5.0ms preprocess, 25.5ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "Speed: 2.8ms preprocess, 57.1ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 41.2ms\n",
      "\n",
      "0: 224x224 ADL 1.00, Fall 0.00, 25.5ms\n",
      "Speed: 4.6ms preprocess, 25.5ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "Speed: 1.4ms preprocess, 41.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 42.6ms\n",
      "\n",
      "0: 224x224 ADL 0.98, Fall 0.02, 29.1ms\n",
      "Speed: 4.0ms preprocess, 29.1ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "Speed: 1.4ms preprocess, 42.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 44.1ms\n",
      "\n",
      "0: 224x224 ADL 0.97, Fall 0.03, 25.7ms\n",
      "Speed: 4.8ms preprocess, 25.7ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "Speed: 2.2ms preprocess, 44.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 42.3ms\n",
      "\n",
      "0: 224x224 ADL 0.97, Fall 0.03, 26.0ms\n",
      "Speed: 4.4ms preprocess, 26.0ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "Speed: 1.9ms preprocess, 42.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 38.3ms\n",
      "\n",
      "0: 224x224 ADL 1.00, Fall 0.00, 25.1ms\n",
      "Speed: 3.5ms preprocess, 25.1ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "Speed: 1.6ms preprocess, 38.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 39.5ms\n",
      "\n",
      "0: 224x224 ADL 0.87, Fall 0.13, 25.8ms\n",
      "Speed: 4.0ms preprocess, 25.8ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "Speed: 1.8ms preprocess, 39.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 40.3ms\n",
      "\n",
      "0: 224x224 ADL 0.80, Fall 0.20, 25.5ms\n",
      "Speed: 4.4ms preprocess, 25.5ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "Speed: 1.7ms preprocess, 40.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 37.5ms\n",
      "\n",
      "0: 224x224 Fall 0.84, ADL 0.16, 21.4ms\n",
      "Speed: 4.8ms preprocess, 21.4ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "Speed: 1.7ms preprocess, 37.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 41.5ms\n",
      "\n",
      "0: 224x224 Fall 0.87, ADL 0.13, 23.6ms\n",
      "Speed: 3.6ms preprocess, 23.6ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "Speed: 1.7ms preprocess, 41.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 40.1ms\n",
      "\n",
      "0: 224x224 ADL 0.58, Fall 0.42, 24.7ms\n",
      "Speed: 3.8ms preprocess, 24.7ms inference, 0.2ms postprocess per image at shape (1, 3, 224, 224)\n",
      "Speed: 1.9ms preprocess, 40.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 40.5ms\n",
      "\n",
      "0: 224x224 ADL 0.79, Fall 0.21, 26.8ms\n",
      "Speed: 3.6ms preprocess, 26.8ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "Speed: 1.3ms preprocess, 40.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 42.9ms\n",
      "\n",
      "0: 224x224 ADL 0.94, Fall 0.06, 24.2ms\n",
      "Speed: 3.7ms preprocess, 24.2ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "Speed: 1.7ms preprocess, 42.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 41.3ms\n",
      "\n",
      "0: 224x224 ADL 0.94, Fall 0.06, 25.1ms\n",
      "Speed: 3.6ms preprocess, 25.1ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "Speed: 1.7ms preprocess, 41.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 41.6ms\n",
      "\n",
      "0: 224x224 ADL 0.87, Fall 0.13, 25.7ms\n",
      "Speed: 3.0ms preprocess, 25.7ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "Speed: 2.1ms preprocess, 41.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 39.2ms\n",
      "Speed: 1.3ms preprocess, 39.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 41.0ms\n",
      "\n",
      "0: 224x224 Fall 0.54, ADL 0.46, 26.3ms\n",
      "Speed: 3.8ms preprocess, 26.3ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "Speed: 1.7ms preprocess, 41.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 40.0ms\n",
      "\n",
      "0: 224x224 Fall 0.80, ADL 0.20, 25.2ms\n",
      "Speed: 4.3ms preprocess, 25.2ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "Speed: 1.8ms preprocess, 40.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 38.5ms\n",
      "Speed: 2.0ms preprocess, 38.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 42.8ms\n",
      "Speed: 1.7ms preprocess, 42.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 43.1ms\n",
      "Speed: 2.2ms preprocess, 43.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 41.6ms\n",
      "Speed: 1.6ms preprocess, 41.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 39.3ms\n",
      "Speed: 2.1ms preprocess, 39.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 39.9ms\n",
      "Speed: 2.3ms preprocess, 39.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 40.5ms\n",
      "Speed: 1.9ms preprocess, 40.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 39.0ms\n",
      "Speed: 2.0ms preprocess, 39.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 37.6ms\n",
      "Speed: 1.5ms preprocess, 37.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 43.9ms\n",
      "Speed: 2.2ms preprocess, 43.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 39.2ms\n",
      "Speed: 2.0ms preprocess, 39.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 39.5ms\n",
      "Speed: 1.5ms preprocess, 39.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 40.7ms\n",
      "Speed: 1.7ms preprocess, 40.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 41.1ms\n",
      "Speed: 2.2ms preprocess, 41.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 42.0ms\n",
      "Speed: 1.7ms preprocess, 42.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 40.7ms\n",
      "Speed: 1.7ms preprocess, 40.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "1ca66048",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T18:12:40.764059Z",
     "start_time": "2025-05-02T18:12:39.728113Z"
    }
   },
   "source": [
    "cap = cv2.VideoCapture(\"C:/Users/Invitado/Documents/fall_detection/data/GMDCSA24/Subject 4/Fall/08.mp4\")\n",
    "model_yolo = YOLO(\"yolo11n.pt\")\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "        # Run YOLO inference on the frame\n",
    "    results = model_yolo(frame, classes=[0], conf = 0.5)\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])  # Bounding box\n",
    "            cropped = frame[y1:y2, x1:x2]  # Recortar persona\n",
    "\n",
    "            if cropped.size == 0:\n",
    "                continue\n",
    "\n",
    "            fall_res = model(cropped, classes=[1], conf = 0.5)\n",
    "            fall_detected = any(r.boxes for r in fall_res)\n",
    "\n",
    "            label = \"FALL\" if fall_detected else \"Normal\"\n",
    "\n",
    "            # Dibujar la caja y el label\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255) if fall_detected else (0, 255, 0), 2)\n",
    "            cv2.putText(frame, label, (x1, y1 - 40), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.9, (0, 0, 255) if fall_detected else (255, 255, 255), 2)\n",
    "\n",
    "    # Visualize the results on the frame\n",
    "    frame = results[0].plot()\n",
    "    # Display the annotated frame\n",
    "    cv2.imshow(\"YOLO Inference\", frame)\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m cap = cv2.VideoCapture(\u001B[33m\"\u001B[39m\u001B[33mC:/Users/Invitado/Documents/fall_detection/data/GMDCSA24/Subject 4/Fall/08.mp4\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      2\u001B[39m model_yolo = YOLO(\u001B[33m\"\u001B[39m\u001B[33myolo11n.pt\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m cap.isOpened():\n",
      "\u001B[31mNameError\u001B[39m: name 'cv2' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "0e92fdc5",
   "metadata": {},
   "source": [
    "frame = cv2.imread(\"C:/Users/Invitado/Documents/fall_detection/data/URFD/Fall/Fall01/Camera/fall-01-cam0-rgb-110.png\")\n",
    "results = model_yolo(frame, classes=[0])\n",
    "for result in results:\n",
    "    for box in result.boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])  # Bounding box\n",
    "        cropped = frame[y1:y2, x1:x2]  # Recortar persona\n",
    "\n",
    "        if cropped.size == 0:\n",
    "            continue\n",
    "\n",
    "        fall_res = model(cropped, classes=[1])\n",
    "        fall_detected = any(r.boxes for r in fall_res)\n",
    "\n",
    "        label = \"FALL\" if fall_detected else \"Normal\"\n",
    "\n",
    "        # Dibujar la caja y el label\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255) if fall_detected else (0, 255, 0), 2)\n",
    "        cv2.putText(frame, label, (x1, y1 - 40), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.9, (0, 0, 255) if fall_detected else (255, 255, 255), 2)\n",
    "\n",
    "# Visualize the results on the frame\n",
    "frame = results[0].plot()\n",
    "# Display the annotated frame\n",
    "plt.imshow(frame)\n",
    "# Break the loop if 'q' is pressed"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
